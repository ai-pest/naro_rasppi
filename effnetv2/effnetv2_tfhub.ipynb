{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# EfficientNetV2 学習・評価・TFLite 化スクリプト"
      ],
      "metadata": {
        "id": "-kmvEuIX30AZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 学習・評価の準備"
      ],
      "metadata": {
        "id": "qVfDt8hnfGmu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 認証\n",
        "\n",
        "Google Cloud Storage にデータセットをアップロードした場合は、次のセルを実行して認証処理を行います。\n",
        "\n",
        "> ️📘ノート\n",
        ">\n",
        "> * データセットを Google Drive にアップロードした場合は、この手順は不要です。"
      ],
      "metadata": {
        "id": "AWFjABnySVbW"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gt_tiRnpIb1i"
      },
      "source": [
        "## EfficientNet-V2 TF-Hub 版\n",
        "## ver. 20220118\n",
        "\n",
        "!mkdir -p /content\n",
        "!gcloud auth application-default login"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Google Drive のマウント\n",
        "\n",
        "Google Drive にデータセットをアップロードした場合は、次のセルを実行して Google Drive を仮想マシンにマウントします。\n",
        "\n",
        "Google Drive は `/content/drive/MyDrive` にマウントされます。\n",
        "\n",
        "> ️📘ノート\n",
        ">\n",
        "> * データセットを Google Cloud Storage にアップロードした場合は、この手順は不要です。"
      ],
      "metadata": {
        "id": "JdIYOcHoexke"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "-gkhy2DGeyAh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zn1Y5ZtCCce2"
      },
      "source": [
        "## 学習・評価の設定"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-11-01T00:30:50.930935Z",
          "iopub.execute_input": "2021-11-01T00:30:50.931472Z",
          "iopub.status.idle": "2021-11-01T00:30:51.687811Z",
          "shell.execute_reply.started": "2021-11-01T00:30:50.931438Z",
          "shell.execute_reply": "2021-11-01T00:30:51.686911Z"
        },
        "trusted": true,
        "id": "S-jq4tjdCce3"
      },
      "source": [
        "#@markdown ## データセットのパス\n",
        "#@markdown - Google Cloud Storage にアップロードした場合\n",
        "#@markdown   - ディレクトリの URL を指定\n",
        "#@markdown   - 例: `gs://mybucket/path/to/dataset_dir`\n",
        "#@markdown - Google Drive にアップロードした場合\n",
        "#@markdown   - ディレクトリパスを指定\n",
        "#@markdown   - 例: `/content/drive/MyDrive/dataset`\n",
        "GCS_PATH = \"/content/drive/MyDrive/dataset\" #@param {type: \"string\"}\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "#@markdown ## 大規模データセットかどうか\n",
        "#@markdown\n",
        "#@markdown 非常に大きなデータセット（画像10万枚以上）では、メモリエラーが発生することがあります。\n",
        "#@markdown\n",
        "#@markdown このチェックボックスを選択すると、メモリエラーを回避する処理を実施します。\n",
        "#\n",
        "# True のときは OOM 回避のために\n",
        "# - 学習時、エポックごとの validation を省略する\n",
        "# - 推論時、遅いが OOM にならない手法を使う\n",
        "HUGE_DATASET = False #@param {type: \"boolean\"}\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "#@markdown ## 学習画像枚数\n",
        "#@markdown \n",
        "#@markdown `train` ディレクトリに含まれる画像数の合計を入力します。\n",
        "NUM_TRAINING_IMAGES = 792 #@param {type: \"number\"}\n",
        "#@markdown ## 推論画像枚数\n",
        "#@markdown \n",
        "#@markdown `validation` ディレクトリに含まれる画像数の合計を入力します。\n",
        "NUM_VALIDATION_IMAGES = 247 #@param {type: \"number\"}\n",
        "NUM_TEST_IMAGES = NUM_VALIDATION_IMAGES\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "# TF-Hub のモデルのパス\n",
        "CORE_LAYER_PATH = 'https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_s/feature_vector/2'\n",
        "\n",
        "# 画像サイズ [H, W]\n",
        "IMAGE_SIZE = [512, 512]\n",
        "# リサイズ方式 (\"RESIZE\": 拡大・縮小, \"PAD\": パディング)\n",
        "RESIZE_METHOD = \"RESIZE\"\n",
        "#@markdown ## バッチサイズ\n",
        "#@markdown \n",
        "#@markdown 1ステップ（1回のモデル更新）で使用する画像の数を指定します。\n",
        "#@markdown \n",
        "#@markdown バッチサイズが大きいほど学習速度が向上しますが、大きすぎるとメモリ不足エラーになります。\n",
        "#@markdown \n",
        "#@markdown 目安は `256` (TPU)、`4` (GPU) です。\n",
        "BATCH_SIZE = 4 #@param {type: \"number\"}\n",
        "# エポック数\n",
        "EPOCHS = 30\n",
        "\n",
        "# 90度ずつ回転する水増し (bool)\n",
        "AUG_ROTATE = True\n",
        "# RandAug (bool)\n",
        "AUG_RANDAUG = False\n",
        "AUG_RANDAUG_N = 2 # レイヤ数\n",
        "AUG_RANDAUG_M = 5 # マグニチュード\n",
        "# > 0.0 ならば Mixup を適用\n",
        "AUG_MIXUP = 0.0\n",
        "# 1.2倍 / 0.8倍の拡大・縮小 (bool)\n",
        "AUG_ZOOM = False\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown\n",
        "#@markdown ## 転移学習元モデルのファイルパス\n",
        "#@markdown\n",
        "#@markdown 転移学習（学習済みのモデルをもとに、別のモデルを学習する）の設定です。\n",
        "#@markdown\n",
        "#@markdown 転移学習しないときは `None` を指定します。\n",
        "#@markdown\n",
        "#@markdown 転移学習を行うときは、転移学習元のモデルファイルのパスを指定します。\n",
        "#@markdown\n",
        "#@markdown * モデルは、データとインデックスの2ファイルに分かれています。  \n",
        "#@markdown   ファイル名の共通する部分を、ファイルパスとして指定します。  \n",
        "#@markdown\n",
        "#@markdown   例: 下記のファイルから転移学習するとき\n",
        "#@markdown      ```\n",
        "#@markdown      ckpt-30.data-00000-of-00001`\n",
        "#@markdown      ckpt-30.index`\n",
        "#@markdown      ```\n",
        "#@markdown   → `\"gs://bucket/path/to/ckpt-30\"` のように指定します。\n",
        "#@markdown * Google Cloud Storage のモデルを参照する場合\n",
        "#@markdown   * モデルのURLを指定\n",
        "#@markdown   * 例: `\"gs://bucket/my_training/ckpt-30\"`\n",
        "#@markdown * Google Drive 上のモデルを参照する場合\n",
        "#@markdown   * モデルのファイルパスを指定\n",
        "#@markdown   * 例: `\"/content/drive/MyDrive/my_training/ckpt-30\"`\n",
        "TRANSFER_LEARNING_FROM = None #@param {type: \"raw\"}\n",
        "# head 層のみ学習するエポック数 (HEAD_EPOCHS <= EPOCHS)\n",
        "HEAD_EPOCHS = 5\n",
        "\n",
        "# 学習率（1cycle）\n",
        "LR_RAMPUP_EPOCHS = 4\n",
        "LR_SUSTAIN_EPOCHS = 0\n",
        "LR_START = 0.00001\n",
        "LR_MAX_BASE = 0.00005   # 実際の Max 学習率は(ノード数)倍になる\n",
        "LR_MIN = 0.00001\n",
        "LR_EXP_DECAY = .8\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "#@markdown ## 学習済みモデルの保存先\n",
        "#@markdown - Google Cloud Storage にモデルを保存する場合\n",
        "#@markdown   - Google Cloud Storage のディレクトリ URL を指定\n",
        "#@markdown   - 例: `gs://mybucket/path/to/dataset_dir`\n",
        "#@markdown   - アクセラレータを TPU にセットする場合は、Cloud Storage に保存する必要があります\n",
        "#@markdown - Google Drive にモデルを保存する場合\n",
        "#@markdown   - ディレクトリパスを指定\n",
        "#@markdown   - 例: `/content/drive/MyDrive/model`\n",
        "LOG_DIR = \"/content/drive/MyDrive/custom_ai/model.tflite\" #@param {type: \"string\"}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2021-11-01T00:29:26.649202Z",
          "iopub.execute_input": "2021-11-01T00:29:26.649614Z",
          "iopub.status.idle": "2021-11-01T00:29:26.657182Z",
          "shell.execute_reply.started": "2021-11-01T00:29:26.649573Z",
          "shell.execute_reply": "2021-11-01T00:29:26.656019Z"
        },
        "trusted": true,
        "id": "L3sf5XL0Ccei"
      },
      "source": [
        "import math, re, os\n",
        "import json\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.metrics \\\n",
        "    import f1_score, precision_score, recall_score, confusion_matrix\n",
        "from functools import partial\n",
        "\n",
        "print(\"Tensorflow version \" + tf.__version__)\n",
        "AUTO = tf.data.experimental.AUTOTUNE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "synset_id2label_path = os.path.join(GCS_PATH, \"synset_id2label.json\") \n",
        "\n",
        "with tf.io.gfile.GFile(synset_id2label_path, \"r\") as f:\n",
        "    synset_id2label = json.loads(f.read())\n",
        "\n",
        "class_id_sorted = sorted(synset_id2label, key=lambda k: int(k))\n",
        "CLASSES = [synset_id2label[k] for k in class_id_sorted]"
      ],
      "metadata": {
        "id": "Gy4wwhz2GefG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### TFRecord のクラスを読み替えるかどうか\n",
        "#REMAP_CLASSES = False\n",
        "#\n",
        "### TFRecord のクラス読み替え用辞書\n",
        "###  - key に TFRecord 作成時のクラス名\n",
        "###  - value に読み替え後のクラス名\n",
        "### を書くと、TFRecord のクラス名を読み替えて学習できます\n",
        "#remap_dict = {\n",
        "#    \"1_fruit_azamiuma\": \"1_fruit_azamiuma\",\n",
        "#    \"1_fruit_healthy\": \"1_fruit_healthy\",\n",
        "#    \"1_fruit_otabakoga\": \"1_fruit_otabakoga\",\n",
        "#    \"1_leaf_aburamushi\": \"aburamushi\",\n",
        "#    \"1_leaf_azamiuma\": \"leaf_azamiuma\",\n",
        "#    \"1_leaf_hamoguribae\": \"leaf_hamoguribae\",\n",
        "#    \"1_leaf_hasumonyoto\": \"leaf_hasumonyoto\",\n",
        "#    \"1_leaf_healthy\": \"leaf_healthy\",\n",
        "#    \"1_leaf_konajirami\": \"konajirami\",\n",
        "#    \"1_leaf_tomatosabidani\": \"1_leaf_tomatosabidani\",\n",
        "#}\n",
        "#\n",
        "### 読み替え用のリストを定義\n",
        "### Remaps labels in TFRecord to the labels given.\n",
        "### Note that labels start from 1, not 0.\n",
        "### e.g.) read_tfrecord(..., relabel=[1,1,2])\n",
        "###   -> The model interprets labels 1 and 2 from TFRecord as label 1\n",
        "###                           label 3 from TFRecord as label 2\n",
        "#if REMAP_CLASSES:\n",
        "#    relabel_list = []\n",
        "#    new_id2label = list(sorted(set(remap_dict.values())))\n",
        "#\n",
        "#    for k in sorted(remap_dict.keys()):\n",
        "#        v = remap_dict[k]\n",
        "#        new_idx = new_id2label.index(v) + 1\n",
        "#        print(f'Old class \"{k}\" is mapped to \"{v}\" ({new_idx})')\n",
        "#        relabel_list.append(new_idx)\n",
        "#\n",
        "#    ## CLASSES を各所で使っている。再定義する\n",
        "#    CLASSES = new_id2label\n",
        "#else:\n",
        "#    print(\"Remapping will not be applied.\")\n",
        "relabel_list = None"
      ],
      "metadata": {
        "id": "bVn4JpjU-cCG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 計算ノード（TPU/GPU/CPU）の判別\n",
        "try: # detect TPUs\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect() # TPU detection\n",
        "    strategy = tf.distribute.TPUStrategy(tpu)\n",
        "except ValueError: # detect GPUs\n",
        "    #strategy = tf.distribute.MirroredStrategy() # for GPU or multi-GPU machines\n",
        "    strategy = tf.distribute.get_strategy() # default strategy that works on CPU and single GPU\n",
        "    #strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy() # for clusters of multi-GPU machines\n",
        "    #raise Exception(\"I find your lack of TPUs disappointing.\")\n",
        "print(\"Number of accelerators: \", strategy.num_replicas_in_sync)\n",
        "\n",
        "## 入力値バリデーション\n",
        "assert HEAD_EPOCHS <= EPOCHS, f'Hey, training the head until epoch {HEAD_EPOCHS}, '\\\n",
        "    f'then all layers until epoch {EPOCHS}, makes no sense.'\n",
        "allowed_resize_methods = (\"RESIZE\", \"PAD\")\n",
        "assert RESIZE_METHOD in allowed_resize_methods, \\\n",
        "    f\"Invalid RESIZE_METHOD: {RESIZE_METHOD}. Choose from {allowed_resize_methods}.\"\n",
        "\n",
        "training_dirpath = os.path.join(GCS_PATH, \"train-*\")\n",
        "TRAINING_FILENAMES = tf.io.gfile.glob(training_dirpath)\n",
        "validation_dirpath = os.path.join(GCS_PATH, \"validation-*\")\n",
        "VALIDATION_FILENAMES = tf.io.gfile.glob(validation_dirpath)\n",
        "TEST_FILENAMES = VALIDATION_FILENAMES\n",
        "\n",
        "def lrfn(epoch):\n",
        "    LR_MAX = LR_MAX_BASE * strategy.num_replicas_in_sync\n",
        "    if epoch < LR_RAMPUP_EPOCHS:\n",
        "        lr = (LR_MAX - LR_START) / LR_RAMPUP_EPOCHS * epoch + LR_START\n",
        "    elif epoch < LR_RAMPUP_EPOCHS + LR_SUSTAIN_EPOCHS:\n",
        "        lr = LR_MAX\n",
        "    else:\n",
        "        lr = (LR_MAX - LR_MIN) * LR_EXP_DECAY**(epoch - LR_RAMPUP_EPOCHS - LR_SUSTAIN_EPOCHS) + LR_MIN\n",
        "    return lr\n",
        "    \n",
        "lr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=True)\n",
        "\n",
        "rng = [i for i in range(EPOCHS)]\n",
        "y = [lrfn(x) for x in rng]\n",
        "plt.plot(rng, y)\n",
        "print(\"Learning rate schedule: {:.3g} to {:.3g} to {:.3g}\".format(y[0], max(y), y[-1]))"
      ],
      "metadata": {
        "id": "Kln1fac1-S6f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNmSkwnZCce6"
      },
      "source": [
        "# データセット"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76mp1DIU4FYO"
      },
      "source": [
        "# データセット\n",
        "## Data augmentation\n",
        "def random_flip(image):\n",
        "    '''画像を水平・垂直反転して返します\n",
        "    '''\n",
        "    image = tf.image.random_flip_left_right(image)\n",
        "    image = tf.image.random_flip_up_down(image)\n",
        "\n",
        "    return image\n",
        "\n",
        "\n",
        "def random_rot90(image):\n",
        "    '''Randomly rotates the image by (n / 2) * pi radian. (n=0,1,2,3)\n",
        "    画像を90度/180度/270度だけ回転して返します\n",
        "    '''\n",
        "    k = tf.random.uniform(shape=[], minval=0, maxval=4, dtype=tf.int32)\n",
        "    image = tf.cond(k >= 1,\n",
        "                    true_fn=lambda: tf.image.rot90(image, k=k),\n",
        "                    false_fn=lambda: image)\n",
        "    return image\n",
        "\n",
        "\n",
        "def sample_beta_distribution(size, concentration_0=0.2, concentration_1=0.2):\n",
        "    gamma_1_sample = tf.random.gamma(shape=[size], alpha=concentration_1)\n",
        "    gamma_2_sample = tf.random.gamma(shape=[size], alpha=concentration_0)\n",
        "    return gamma_1_sample / (gamma_1_sample + gamma_2_sample)\n",
        "\n",
        "\n",
        "def mix_up(ds_one, ds_two, alpha=0.2):\n",
        "    # Unpack two datasets\n",
        "    images_one, labels_one = ds_one\n",
        "    images_two, labels_two = ds_two\n",
        "    batch_size = tf.shape(images_one)[0]\n",
        "\n",
        "    # Sample lambda and reshape it to do the mixup\n",
        "    l = sample_beta_distribution(batch_size, alpha, alpha)\n",
        "    x_l = tf.reshape(l, (batch_size, 1, 1, 1))\n",
        "    y_l = tf.reshape(l, (batch_size, 1))\n",
        "\n",
        "    # Perform mixup on both images and labels by combining a pair of images/labels\n",
        "    # (one from each dataset) into one image/label\n",
        "    images = images_one * x_l + images_two * (1 - x_l)\n",
        "    labels = labels_one * y_l + labels_two * (1 - y_l)\n",
        "    return (images, labels)\n",
        "\n",
        "\n",
        "def randaug(image, num_layers=2, magnitude=15):\n",
        "    '''RandAugment を適用します\n",
        "    (本家 EffNet-V2 から流用した TensorFlow 純正の実装)\n",
        "    Args:\n",
        "        image: `tf.Tensor`, 値域 [0,1]\n",
        "        num_layers: `int`, レイヤ数\n",
        "        magnitude: `int`, 画像効果の強度\n",
        "    Returns:\n",
        "        `tf.Tensor` 形式の画像, 値域 [0,1]\n",
        "    '''\n",
        "    print(\"Applying RandAugment.\")\n",
        "    image *= 255\n",
        "    input_image_type = image.dtype\n",
        "    image = tf.clip_by_value(image, 0.0, 255.0)\n",
        "    image = tf.cast(image, dtype=tf.uint8)  # float では???\n",
        "    image = autoaugment.distort_image(\n",
        "        image, aug_name='randaug', ra_num_layers=num_layers, \n",
        "        ra_magnitude=magnitude)\n",
        "    image = tf.cast(image, dtype=input_image_type)\n",
        "    image /= 255\n",
        "\n",
        "    return image\n",
        "\n",
        "\n",
        "def zoom(image):\n",
        "    \"\"\"Zooms the image by factor of 1.2.\"\"\"\n",
        "    shape = tf.shape(image)\n",
        "    ## Zoom by (about) 1.2x\n",
        "    image = tf.image.central_crop(image, 0.8)\n",
        "    ## Then crop\n",
        "    image = tf.image.resize(image, shape[:2])\n",
        "\n",
        "    return image\n",
        "\n",
        "\n",
        "def mooz(image):\n",
        "    \"\"\"Moozes the image by factor of 0.8.\"\"\"\n",
        "    shape = tf.shape(image)\n",
        "    s0 = tf.cast(shape[0], tf.float32)\n",
        "    s1 = tf.cast(shape[1], tf.float32)\n",
        "    ## Increases the image size by 1.2x with pad\n",
        "    image = tf.image.pad_to_bounding_box(\n",
        "        image,\n",
        "        offset_height=tf.cast(s0*0.1, tf.int32),\n",
        "        offset_width=tf.cast(s1*0.1, tf.int32),\n",
        "        target_height=tf.cast(s0*1.2, tf.int32),\n",
        "        target_width=tf.cast(s1*1.2, tf.int32))\n",
        "    ## Then resizes to the original shape\n",
        "    image = tf.image.resize(image, shape[:2])\n",
        "\n",
        "    return image\n",
        "\n",
        "\n",
        "def random_zoom_mooz(image):\n",
        "    '''画像を1/3の確率で拡大切り抜き、縮小して、またはそのまま返します\n",
        "    拡大・縮小の中心点は、画像の中心です\n",
        "    Args:\n",
        "        image: `tf.Tensor`\n",
        "    Returns:\n",
        "        `tf.Tensor` 形式の画像、幅と高さは image と同じ\n",
        "    '''\n",
        "    k = tf.random.uniform(shape=[], minval=0, maxval=3, dtype=tf.int32)\n",
        "    image = tf.cond(k == 1,\n",
        "                    true_fn=lambda: zoom(image),\n",
        "                    false_fn=lambda: image)\n",
        "    image = tf.cond(k == 2,\n",
        "                    true_fn=lambda: mooz(image),\n",
        "                    false_fn=lambda: image)\n",
        "\n",
        "    return image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "execution": {
          "iopub.status.busy": "2021-11-01T00:31:12.854328Z",
          "iopub.execute_input": "2021-11-01T00:31:12.854603Z",
          "iopub.status.idle": "2021-11-01T00:31:12.87609Z",
          "shell.execute_reply.started": "2021-11-01T00:31:12.854576Z",
          "shell.execute_reply": "2021-11-01T00:31:12.875259Z"
        },
        "trusted": true,
        "id": "o7J18GdwCce7"
      },
      "source": [
        "def decode_image(image):\n",
        "    image = tf.image.decode_jpeg(image, channels=3)\n",
        "    image = tf.cast(image, tf.float32)\n",
        "\n",
        "    ## RESIZE_METHOD に応じて、サイズ合わせの方法（パディング/リサイズ）を切り替え\n",
        "    if RESIZE_METHOD == \"PAD\":\n",
        "        image = tf.image.resize_with_pad(\n",
        "            image, IMAGE_SIZE[0], IMAGE_SIZE[1], \n",
        "            method=tf.image.ResizeMethod.BICUBIC)\n",
        "    else:\n",
        "        image = tf.image.resize(\n",
        "            image, IMAGE_SIZE, method=tf.image.ResizeMethod.BICUBIC)\n",
        "\n",
        "    return image\n",
        "\n",
        "\n",
        "def read_tfrecord(example, labeled, one_hot=False, relabel=None):\n",
        "    '''Parses Google AutoML-style TFRecord dataset.\n",
        "    Args:\n",
        "        example\n",
        "        labeled\n",
        "        one_hot\n",
        "        relabel: `list`. Remaps labels in TFRecord to the labels given.\n",
        "            Note that labels start from 1, not 0.\n",
        "            e.g.) read_tfrecord(..., relabel=[1,1,2])\n",
        "              -> The model interprets labels 1 and 2 from TFRecord as label 1\n",
        "                                      label 3 from TFRecord as label 2\n",
        "    '''\n",
        "    tfrecord_format = (\n",
        "        {\n",
        "            \"image/encoded\": tf.io.FixedLenFeature([], tf.string),\n",
        "            \"image/class/label\": tf.io.FixedLenFeature([], tf.int64),\n",
        "        }\n",
        "        if labeled\n",
        "        else {\"image\": tf.io.FixedLenFeature([], tf.string),}\n",
        "    )\n",
        "    example = tf.io.parse_single_example(example, tfrecord_format)\n",
        "    image = decode_image(example[\"image/encoded\"])\n",
        "    image /= 255\n",
        "    if labeled:\n",
        "        label = tf.cast(example[\"image/class/label\"], tf.int64)\n",
        "        if relabel is not None:\n",
        "            label -= 1  # Since (image/class/label == 1) maps to relabel[0] \n",
        "            relabel_t = tf.constant(relabel, tf.float32)\n",
        "            label = tf.gather(relabel_t, label)\n",
        "        label -= 1\n",
        "        if one_hot:\n",
        "            print(\"One hot is active!\")\n",
        "            label = tf.cast(label, tf.uint8)\n",
        "            label = tf.one_hot(label, len(CLASSES))\n",
        "            label = tf.cast(label, tf.float32)\n",
        "        return image, label\n",
        "    return image\n",
        "\n",
        "\n",
        "def load_dataset(\n",
        "        filenames, labeled=True, ordered=False, one_hot=False, relabel=None):\n",
        "    ignore_order = tf.data.Options()\n",
        "    if not ordered:\n",
        "        ignore_order.experimental_deterministic = False  \n",
        "            # disable order, increase speed\n",
        "\n",
        "    dataset = tf.data.TFRecordDataset(filenames)  \n",
        "        # automatically interleaves reads from multiple files\n",
        "    dataset = dataset.with_options(ignore_order)\n",
        "        # uses data as soon as it streams in, rather than in its original order\n",
        "    dataset = dataset.map(\n",
        "        partial(\n",
        "            read_tfrecord, labeled=labeled, one_hot=one_hot, relabel=relabel),\n",
        "        num_parallel_calls=AUTO)\n",
        "        # returns a dataset of (image, label) pairs if labeled=True \n",
        "        # or just images if labeled=False\n",
        "\n",
        "    return dataset\n",
        "\n",
        "\n",
        "def _get_training_dataset(\n",
        "    aug_rotate=False, one_hot=False, aug_randaug=False, aug_zoom=False,\n",
        "    randaug_num_layers=2, randaug_magnitude=15, relabel=None):\n",
        "    '''Obtain the training datset.\n",
        "    Args:\n",
        "      aug_rotate: `bool`. If true, performs random rotation and flip.\n",
        "      one_hot: `bool`, True ならばワンホット符号化する（Mixup用）\n",
        "      aug_randaug: `bool`. True ならば、RandAugment を適用する\n",
        "      aug_zoom: `bool`. True ならば、拡大・縮小水増しを適用する\n",
        "      randaug_num_layers: `int`, RandAugment のレイヤ数\n",
        "      randaug_magnitude: `int`, RandAugment の画像効果の強度\n",
        "      relabel: `dict` or None, `dict` ならば TFRecord のラベルを読み替える\n",
        "    '''\n",
        "    dataset = load_dataset(\n",
        "        TRAINING_FILENAMES, labeled=True, one_hot=one_hot, relabel=relabel)\n",
        "\n",
        "    if aug_rotate:\n",
        "        dataset = dataset.map(\n",
        "            lambda x, y: (random_flip(x), y),\n",
        "            num_parallel_calls=AUTO)\n",
        "        dataset = dataset.map(\n",
        "            lambda x, y: (random_rot90(x), y), \n",
        "            num_parallel_calls=AUTO)\n",
        "\n",
        "    if aug_randaug:\n",
        "        dataset = dataset.map(\n",
        "            lambda x, y: (\n",
        "                randaug(\n",
        "                    x, num_layers=randaug_num_layers, \n",
        "                    magnitude=randaug_magnitude),\n",
        "                y), \n",
        "            num_parallel_calls=AUTO)\n",
        "\n",
        "    if aug_zoom:\n",
        "        dataset = dataset.map(\n",
        "            lambda x, y: (random_zoom_mooz(x), y),\n",
        "            num_parallel_calls=AUTO)\n",
        "\n",
        "    dataset = dataset.repeat() # the training dataset must repeat for several epochs\n",
        "    dataset = dataset.shuffle(2048)\n",
        "    dataset = dataset.batch(BATCH_SIZE)\n",
        "    dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)\n",
        "\n",
        "    return dataset\n",
        "\n",
        "\n",
        "def get_training_dataset(aug_mixup=0.0, **kwargs):\n",
        "    '''Wraps _get_training_dataset to add mixup functionality.\n",
        "    Args:\n",
        "      aug_mixup  `float`. If > 0.0, performs mixup augmentation.\n",
        "      **kwargs   See the definition of _get_training_dataset().'''\n",
        "    if aug_mixup == 0.0:\n",
        "        print('Mixup is disabled.')\n",
        "        return _get_training_dataset(**kwargs)\n",
        "    else:\n",
        "        print('Mixup is enabled.')\n",
        "        dataset_1 = _get_training_dataset(one_hot=True, **kwargs)\n",
        "        dataset_2 = _get_training_dataset(one_hot=True, **kwargs)\n",
        "        train_ds = tf.data.Dataset.zip((dataset_1, dataset_2))\n",
        "        train_ds = train_ds.map(\n",
        "            lambda ds1, ds2: mix_up(ds1, ds2, alpha=aug_mixup),\n",
        "            num_parallel_calls=AUTO)\n",
        "        return train_ds\n",
        "\n",
        "    \n",
        "def get_validation_dataset(ordered=False, one_hot=False, relabel=None):\n",
        "    dataset = load_dataset(\n",
        "        VALIDATION_FILENAMES, labeled=True, ordered=ordered, one_hot=one_hot,\n",
        "        relabel=relabel)\n",
        "    dataset = dataset.batch(BATCH_SIZE)\n",
        "    dataset = dataset.cache()\n",
        "    dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)\n",
        "    return dataset\n",
        "\n",
        "\n",
        "def get_test_dataset(ordered=False, relabel=None):\n",
        "    dataset = load_dataset(\n",
        "        TEST_FILENAMES, labeled=False, ordered=ordered, relabel=relabel)\n",
        "    dataset = dataset.batch(BATCH_SIZE)\n",
        "    dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)\n",
        "    return dataset\n",
        "\n",
        "NUM_TEST_IMAGES = NUM_VALIDATION_IMAGES\n",
        "STEPS_PER_EPOCH = NUM_TRAINING_IMAGES // BATCH_SIZE\n",
        "VALIDATION_STEPS = -(-NUM_VALIDATION_IMAGES // BATCH_SIZE) # The \"-(-//)\" trick rounds up instead of down :-)\n",
        "TEST_STEPS = -(-NUM_TEST_IMAGES // BATCH_SIZE)             # The \"-(-//)\" trick rounds up instead of down :-)\n",
        "print('Dataset: {} training images, {} validation images, {} unlabeled test images'.format(NUM_TRAINING_IMAGES, NUM_VALIDATION_IMAGES, NUM_TEST_IMAGES))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FeZmzk7XCcfA"
      },
      "source": [
        "# モデル定義"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_hToqYke6eil"
      },
      "source": [
        "## Custom callbacks, from the original EffNet-V2\n",
        "## \n",
        "class ReusableBackupAndRestore(tf.keras.callbacks.BackupAndRestore):\n",
        "  \"\"\"A BackupAndRestore callback that can be used across multiple model.fit()s.\"\"\"\n",
        "\n",
        "  def on_train_end(self, logs=None):\n",
        "    # don't delete the backup, so it can be used for future model.fit()s\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-11-01T00:32:04.957262Z",
          "iopub.execute_input": "2021-11-01T00:32:04.958165Z",
          "iopub.status.idle": "2021-11-01T00:32:40.403626Z",
          "shell.execute_reply.started": "2021-11-01T00:32:04.958108Z",
          "shell.execute_reply": "2021-11-01T00:32:40.402712Z"
        },
        "trusted": true,
        "id": "ybid_m_7CcfA"
      },
      "source": [
        "with strategy.scope():\n",
        "    # TF>=2.4: load models from TF Hub directly to the TPU\n",
        "    load_locally = tf.saved_model.LoadOptions(\n",
        "        experimental_io_device='/job:localhost')\n",
        "    pretrained_model = hub.KerasLayer(\n",
        "        CORE_LAYER_PATH, trainable=True, input_shape=[*IMAGE_SIZE, 3], \n",
        "        load_options=load_locally)\n",
        "    model = tf.keras.Sequential([\n",
        "        # the expected image format for all TFHub image models is float32 in\n",
        "        # [0,1) range\n",
        "        tf.keras.layers.Lambda(\n",
        "            lambda data: tf.image.convert_image_dtype(data, tf.float32), \n",
        "            input_shape=[*IMAGE_SIZE, 3]),\n",
        "        pretrained_model,\n",
        "        tf.keras.layers.Dense(len(CLASSES), activation='softmax')\n",
        "    ])\n",
        "\n",
        "    # Optionally make the core layer untrainable (train only the head).\n",
        "    if HEAD_EPOCHS >= 1:\n",
        "        model.layers[1].trainable = False\n",
        "\n",
        "    ## Optionally initialize the model with pretrained data.\n",
        "    if TRANSFER_LEARNING_FROM is not None:\n",
        "        ## Remove the last layer, load weights, then attach a fresh head.\n",
        "        model.pop()\n",
        "        model.load_weights(TRANSFER_LEARNING_FROM)\n",
        "        model.add(tf.keras.layers.Dense(len(CLASSES), activation='softmax'))\n",
        "\n",
        "\n",
        "## 損失関数: Mixupあり（One-hot）の場合は categorical_crossentropy\n",
        "##          Mixupなしの場合は sparse_categorical_crossentropy\n",
        "loss_fn = 'categorical_crossentropy' if AUG_MIXUP > 0 \\\n",
        "          else 'sparse_categorical_crossentropy'\n",
        "print(f\"Loss function: {loss_fn}\")\n",
        "\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss=loss_fn,\n",
        "    metrics=['accuracy'],\n",
        "    steps_per_execution=16,\n",
        ")\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MtbYWhj3CcfA"
      },
      "source": [
        "# 学習"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = get_training_dataset(\n",
        "    aug_rotate=AUG_ROTATE, aug_mixup=AUG_MIXUP, aug_randaug=AUG_RANDAUG, \n",
        "    aug_zoom=AUG_ZOOM, randaug_num_layers=AUG_RANDAUG_N, \n",
        "    randaug_magnitude=AUG_RANDAUG_M, relabel=relabel_list)\n",
        "\n",
        "callbacks = [\n",
        "    lr_callback,\n",
        "    tf.keras.callbacks.ModelCheckpoint(\n",
        "        os.path.join(LOG_DIR, 'ckpt-{epoch:d}'), verbose=1,\n",
        "        save_weights_only=True),\n",
        "    tf.keras.callbacks.TensorBoard(log_dir=LOG_DIR, update_freq=100),\n",
        "    ReusableBackupAndRestore(backup_dir=LOG_DIR),\n",
        "]\n",
        "\n",
        "## 大規模データセットのときは、学習時の validation を省略して OOM 回避\n",
        "if HUGE_DATASET:\n",
        "    args_validation = {}\n",
        "else:\n",
        "    args_validation = {\n",
        "        \"validation_data\": get_validation_dataset(one_hot=(AUG_MIXUP>0.0)),\n",
        "        \"validation_steps\": VALIDATION_STEPS,\n",
        "    }\n",
        "\n",
        "## HEAD_EPOCHS が 1 以上のときは\n",
        "##  - 先に head 層を HEAD_EPOCHS まで学習する\n",
        "##  - 全レイヤを学習可能にして、再度学習する\n",
        "if HEAD_EPOCHS >= 1:\n",
        "    history = model.fit(\n",
        "        train_ds, steps_per_epoch=STEPS_PER_EPOCH, epochs=HEAD_EPOCHS, \n",
        "        callbacks=callbacks, verbose=2, **args_validation)\n",
        "    model.layers[1].trainable = True\n",
        "    model.summary()\n",
        "    model.compile(\n",
        "        optimizer='adam',\n",
        "        loss=loss_fn,\n",
        "        metrics=['accuracy'],\n",
        "        steps_per_execution=16,\n",
        "        )\n",
        "\n",
        "history = model.fit(\n",
        "    train_ds, steps_per_epoch=STEPS_PER_EPOCH, epochs=EPOCHS, \n",
        "    callbacks=callbacks, verbose=2, **args_validation)"
      ],
      "metadata": {
        "id": "5voVx17o1hhd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5PwktbhnCcfC"
      },
      "source": [
        "# 学習済みモデルの評価"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oCbDf37uDCv5"
      },
      "source": [
        "final_ckpt_path = os.path.join(LOG_DIR, f'ckpt-{EPOCHS}')\n",
        "model.load_weights(final_ckpt_path)\n",
        "\n",
        "if not HUGE_DATASET:\n",
        "    cmdataset = get_validation_dataset(ordered=True) # since we are splitting the dataset and iterating separately on images and labels, order matters.\n",
        "    images_ds = cmdataset.map(lambda image, label: image)\n",
        "    labels_ds = cmdataset.map(lambda image, label: label).unbatch()\n",
        "    cm_correct_labels = next(iter(labels_ds.batch(NUM_VALIDATION_IMAGES))).numpy() # get everything as one batch\n",
        "    cm_probabilities = model.predict(images_ds, steps=VALIDATION_STEPS)\n",
        "    cm_predictions = np.argmax(cm_probabilities, axis=-1)\n",
        "    cm_predictions = cm_predictions[:NUM_VALIDATION_IMAGES] \n",
        "        # Lest some validation data are counted twice\n",
        "    #print(\"Correct   labels: \", cm_correct_labels.shape, cm_correct_labels)\n",
        "    #print(\"Predicted labels: \", cm_predictions.shape, cm_predictions)\n",
        "\n",
        "if HUGE_DATASET:\n",
        "    import keras.backend as K\n",
        "    from tqdm import tqdm\n",
        "\n",
        "    ## 巨大（画像数万枚、画像サイズ1408）な tf.Dataset で推論すると OOM になる\n",
        "    ## - GPU ではホストのRAMを使い果たしてクラッシュする\n",
        "    ## - TPU では `Socket closed` エラーが起こる\n",
        "    ## 関連？: https://github.com/keras-team/keras/issues/13118\n",
        "    ##\n",
        "    ## 対策として、tf.Dataset を model に渡さず、\n",
        "    ## ホストVM側でデータセットを少しずつ読みだして (`model(x)`)、TPU に推論させる\n",
        "    ## 動作速度が遅くなるが、推論はできる\n",
        "    ## 注意: GPU では OOM になる\n",
        "    BS = 64\n",
        "    TAKE = int((NUM_VALIDATION_IMAGES // BS) * 1)\n",
        "    data_len = BS * TAKE\n",
        "\n",
        "    cm_correct_labels = np.empty((data_len), dtype=int)    # Stores GTs\n",
        "    cm_predictions = np.empty((data_len), dtype=int)       # Stores preds\n",
        "\n",
        "    cmdataset = tf.data.TFRecordDataset(VALIDATION_FILENAMES)\n",
        "    dataset = cmdataset.map(\n",
        "        partial(\n",
        "            read_tfrecord, labeled=True, one_hot=False, relabel=relabel_list),\n",
        "        num_parallel_calls=AUTO)\n",
        "    \n",
        "    for i, batch in tqdm(\n",
        "            enumerate(dataset.shuffle(100).batch(BS).take(TAKE)), total=TAKE):\n",
        "        images_batch, labels_batch = batch\n",
        "        images_batch = tf.squeeze(images_batch)\n",
        "            # (bs, 1, h, w, c) -> (bs, h, w, c)\n",
        "        prob_y = model(images_batch, training=False) \n",
        "            # since model.predict() causes OOM\n",
        "        pred_y = tf.math.argmax(prob_y, axis=-1)\n",
        "        labels_batch = tf.cast(labels_batch, tf.int8)\n",
        "        labels_batch = tf.squeeze(labels_batch)\n",
        "\n",
        "        # 結果を保存\n",
        "        idx_from = i * BS\n",
        "        idx_to = idx_from + BS\n",
        "        cm_correct_labels[idx_from:idx_to] = labels_batch\n",
        "        cm_predictions[idx_from:idx_to] = pred_y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "daKfKXe4CcfC"
      },
      "source": [
        "## F値などの算出\n",
        "cmat = confusion_matrix(cm_correct_labels, cm_predictions, labels=range(len(CLASSES)))\n",
        "score = f1_score(cm_correct_labels, cm_predictions, labels=range(len(CLASSES)), average='macro')\n",
        "precision = precision_score(cm_correct_labels, cm_predictions, labels=range(len(CLASSES)), average='macro')\n",
        "recall = recall_score(cm_correct_labels, cm_predictions, labels=range(len(CLASSES)), average='macro')\n",
        "cmat = (cmat.T / cmat.sum(axis=1)).T # normalized\n",
        "#display_confusion_matrix(cmat, score, precision, recall)\n",
        "print('f1 score: {:.3f}, precision: {:.3f}, recall: {:.3f}'.format(score, precision, recall))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gR4dSE1jWFc0"
      },
      "source": [
        "## 混同行列をCSV形式で出力\n",
        "import pandas as pd\n",
        "cmat = confusion_matrix(\n",
        "    cm_correct_labels, cm_predictions, \n",
        "    labels=range(len(np.unique(cm_correct_labels))))\n",
        "df = pd.DataFrame(cmat, index=CLASSES, columns=CLASSES)\n",
        "print(df.to_csv())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Top-1精度の算出\n",
        "top_1_acc = (cm_correct_labels == cm_predictions).sum() / len(cm_correct_labels)\n",
        "print(f'top_1_accuracy: {top_1_acc * 100:.1f}%')"
      ],
      "metadata": {
        "id": "qGv-meaOryRj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 学習済みモデルの変換\n",
        "\n",
        "学習済みの画像識別モデルを、エッジ端末で動作可能な形式（TFLite）に変換します。\n",
        "\n",
        "変換により、識別性能はやや低下します。\n",
        "\n",
        "> ⚠️注意⚠️\n",
        "> \n",
        "> アクセラレータの設定が TPU のときに変換を試みると、`converter.convert()` の部分でエラーになります：\n",
        ">\n",
        "> ```\n",
        "> ValueError: Device /job:worker/replica:0/task:0/device:CPU:0 is not found\n",
        "> ```\n",
        ">\n",
        "> アクセラレータの設定が TPU になっている場合は、**アクセラレータをTPU以外に設定してVMを再起動し、学習・評価以外の各セルを再実行してから**下記のコードを実行してください。\n"
      ],
      "metadata": {
        "id": "NQq4_rBQlh0T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ## TFLite モデルの書き出し先\n",
        "#@markdown\n",
        "#@markdown 書き出すファイルのパスを指定します。\n",
        "#@markdown\n",
        "#@markdown Google Drive をマウントしている場合は、`/content/drive/MyDrive/` 以下のファイルパスを指定することで、Google Drive にモデルを保存できます。\n",
        "#@markdown\n",
        "#@markdown 例: `/content/drive/MyDrive/model.tflite` (Google Drive に書き出す場合)\n",
        "EXPORT_TFLITE_TO = \"/content/drive/MyDrive/model.tflite\" #@param {type: \"string\"}"
      ],
      "metadata": {
        "id": "DX13CN2gW2vo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_ckpt_path = os.path.join(LOG_DIR, f'ckpt-{EPOCHS}')\n",
        "print(f\"Loading {final_ckpt_path}\")\n",
        "model.load_weights(final_ckpt_path)"
      ],
      "metadata": {
        "id": "XcvwqNCHCllc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "from pathlib import Path\n",
        "\n",
        "## パラメータの設定\n",
        "optimize_lite_model = True\n",
        "quantize_lite_model = True\n",
        "num_calibration_examples = 100\n",
        "tflite_export_path = Path(EXPORT_TFLITE_TO)\n",
        "\n",
        "representative_dataset = None\n",
        "\n",
        "## validation データを使用して、軽量化モデルを最適化する\n",
        "if optimize_lite_model and num_calibration_examples:\n",
        "  # Use a bounded number of training examples without labels for calibration.\n",
        "  # TFLiteConverter expects a list of input tensors, each with batch size 1.\n",
        "  representative_dataset = lambda: itertools.islice(\n",
        "      ([image[None, ...]] for batch, _ in get_validation_dataset() for image in batch),\n",
        "      num_calibration_examples)\n",
        "\n",
        "## TFLite モデルの変換\n",
        "#converter = tf.lite.TFLiteConverter.from_saved_model(LOG_DIR)\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "if optimize_lite_model:\n",
        "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "if quantize_lite_model:\n",
        "    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "    converter.inference_input_type = tf.int8\n",
        "    converter.inference_output_type = tf.int8\n",
        "if representative_dataset:  # This is optional, see above.\n",
        "    converter.representative_dataset = representative_dataset\n",
        "lite_model_content = converter.convert() # 3～4分かかる\n",
        "\n",
        "## 作成した TFLite モデルの書き出し\n",
        "tflite_export_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "with open(tflite_export_path, \"wb\") as f:\n",
        "    f.write(lite_model_content)\n",
        "    print(\"Wrote %sTFLite model of %d bytes.\" %\n",
        "        (\"optimized \" if optimize_lite_model else \"\", len(lite_model_content)))"
      ],
      "metadata": {
        "id": "ublUnrdEk9RK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## TFLite モデルのシグネチャ（入出力テンソル）のデータ確認\n",
        "#interpreter = tf.lite.Interpreter(model_content=lite_model_content)\n",
        "## This little helper wraps the TF Lite interpreter as a numpy-to-numpy function.\n",
        "#\n",
        "#input_details = interpreter.get_input_details()\n",
        "#output_details = interpreter.get_output_details()\n",
        "#\n",
        "#print(input_details)\n",
        "#print(output_details)\n",
        "#\n",
        "#def lite_model(images):\n",
        "#  interpreter.allocate_tensors()\n",
        "#  interpreter.set_tensor(interpreter.get_input_details()[0]['index'], images)\n",
        "#  interpreter.invoke()\n",
        "#  return interpreter.get_tensor(interpreter.get_output_details()[0]['index'])"
      ],
      "metadata": {
        "id": "1cF9UGlLlFDc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LICENSE\n",
        "\n",
        "Copyright 2021 MARTIN GÖRNER\n",
        "\n",
        "Copyright 2022 Sailaja Raja and Ronil Modi\n",
        "\n",
        "Copyright 2021 Google Inc. All rights reserved.\n",
        "\n",
        "Copyright 2021 The TensorFlow Hub Authors. All rights reserved.\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.\n",
        "\n",
        "\n",
        "Original files:\n",
        "\n",
        "* https://www.kaggle.com/code/rajasailaja/test-tpu\n",
        "* https://www.kaggle.com/code/mgornergoogle/efficientnetb7-on-100-flowers\n",
        "* https://github.com/GoogleCloudPlatform/practical-ml-vision-book/blob/d617988b1cc7de73ec11a6015fd1983a6925af5e/03_image_models/03b_finetune_experiment_lr_decay_xception_flowers104.ipynb\n",
        "* https://github.com/tensorflow/hub/blob/12b09be736893efd11a0a54d85e0e01516f44442/examples/colab/tf2_image_retraining.ipynb\n",
        "* https://www.tensorflow.org/hub/tutorials/tf2_image_retraining\n",
        "\n",
        "Major changes from the original:\n",
        "\n",
        "* Combined the above files to create a train-evaluation-quantization pipleline of EfficientV2 models"
      ],
      "metadata": {
        "id": "PTdEtsTYlgJB"
      }
    }
  ]
}